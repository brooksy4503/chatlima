{
  "$schema": "https://json-schema.org/draft/2019-09/schema",
  "description": "List of AI models that have been tested and found to have non-working endpoints",
  "lastUpdated": "2025-12-07T09:14:09.140Z",
  "models": {
    "google/gemini-2.5-pro-exp-03-25": {
      "provider": "openrouter",
      "reason": "No endpoints found",
      "lastTested": "2024-01-01T00:00:00.000Z",
      "testError": "404 - Model not found",
      "retryCount": 0,
      "manuallyBlocked": true
    },
    "coding/gemini-2.5-flash-preview-05-20": {
      "provider": "openrouter",
      "reason": "Added to blocklist - endpoint issues",
      "lastTested": "2024-01-01T00:00:00.000Z",
      "testError": "Connection timeout",
      "retryCount": 0,
      "manuallyBlocked": true
    },
    "google/gemini-2.5-flash-preview-05-20": {
      "provider": "requesty",
      "reason": "Requesty version - no valid endpoint",
      "lastTested": "2024-01-01T00:00:00.000Z",
      "testError": "Invalid model configuration",
      "retryCount": 0,
      "manuallyBlocked": true
    },
    "openai/o3-pro": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on requests per min (RPM): Limit 0, Requested 1. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-08-07T09:40:08.266Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on requests per min (RPM): Limit 0, Requested 1. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 3,
      "manuallyBlocked": false
    },
    "openai/o3": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-08-07T09:40:02.683Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 3,
      "manuallyBlocked": false
    },
    "nvidia/llama-3.1-nemotron-ultra-253b-v1:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:39.837Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "rekaai/reka-flash-3:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: RekaAI/reka-flash-3\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:41.737Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: RekaAI/reka-flash-3\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "rekaai/reka-flash-3": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"No endpoints found for rekaai/reka-flash-3.\",\"code\":404},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:43.092Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"No endpoints found for rekaai/reka-flash-3.\",\"code\":404},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "deepseek/deepseek-r1-distill-qwen-14b:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:48.619Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "qwen/qwq-32b-preview": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"object\\\":\\\"error\\\",\\\"message\\\":\\\"Only Qwen/Qwen2.5-72B-Instruct && Qwen/Qwen2.5-VL-72B-Instruct && deepseek-ai/DeepSeek-V3-0324 && meta-llama/Meta-Llama-3.1-70B-Instruct && meta-llama/Llama-3.3-70B-Instruct && meta-llama/Llama-3.2-3B-Instruct && FLUX.1-dev && StableDiffusion && meta-llama/Meta-Llama-3.1-8B-Instruct && tt && test && deepseek-ai/DeepSeek-R1 && moonshotai/Kimi-K2-Instruct && meta-llama/Meta-Llama-3-70B-Instruct && Qwen/Qwen2.5-VL-7B-Instruct && Qwen/Qwen3-235B-A22B && meta-llama/Meta-Llama-3.1-405B-Instruct && Qwen/QwQ-32B && deepseek-ai/DeepSeek-V3 && Qwen/Qwen3-235B-A22B-Instruct-2507 && NousResearch/Hermes-3-Llama-3.1-70B && meta-llama/Meta-Llama-3.1-405B && Qwen/Qwen3-Coder-480B-A35B-Instruct && mistralai/Pixtral-12B-2409 && Qwen/Qwen2.5-Coder-32B-Instruct && TTS && openai/gpt-oss-20b && deepseek-ai/DeepSeek-R1-0528 && openai/gpt-oss-120b allowed now, your model Qwen/QwQ-32B-Preview\\\",\\\"type\\\":\\\"\\\",\\\"param\\\":null,\\\"code\\\":40301}\",\"provider_name\":\"Hyperbolic\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:52.719Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"object\\\":\\\"error\\\",\\\"message\\\":\\\"Only Qwen/Qwen2.5-72B-Instruct && Qwen/Qwen2.5-VL-72B-Instruct && deepseek-ai/DeepSeek-V3-0324 && meta-llama/Meta-Llama-3.1-70B-Instruct && meta-llama/Llama-3.3-70B-Instruct && meta-llama/Llama-3.2-3B-Instruct && FLUX.1-dev && StableDiffusion && meta-llama/Meta-Llama-3.1-8B-Instruct && tt && test && deepseek-ai/DeepSeek-R1 && moonshotai/Kimi-K2-Instruct && meta-llama/Meta-Llama-3-70B-Instruct && Qwen/Qwen2.5-VL-7B-Instruct && Qwen/Qwen3-235B-A22B && meta-llama/Meta-Llama-3.1-405B-Instruct && Qwen/QwQ-32B && deepseek-ai/DeepSeek-V3 && Qwen/Qwen3-235B-A22B-Instruct-2507 && NousResearch/Hermes-3-Llama-3.1-70B && meta-llama/Meta-Llama-3.1-405B && Qwen/Qwen3-Coder-480B-A35B-Instruct && mistralai/Pixtral-12B-2409 && Qwen/Qwen2.5-Coder-32B-Instruct && TTS && openai/gpt-oss-20b && deepseek-ai/DeepSeek-R1-0528 && openai/gpt-oss-120b allowed now, your model Qwen/QwQ-32B-Preview\\\",\\\"type\\\":\\\"\\\",\\\"param\\\":null,\\\"code\\\":40301}\",\"provider_name\":\"Hyperbolic\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "x-ai/grok-vision-beta": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"code\\\":\\\"Some requested entity was not found\\\",\\\"error\\\":\\\"The model grok-vision-beta does not exist or your team bb642ce4-5161-45c9-8f34-408850883602 does not have access to it. Please ensure you're using the correct API key. If you believe this is a mistake, please contact support and quote your team ID and the model name.\\\"}\",\"provider_name\":\"xAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:54.360Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"code\\\":\\\"Some requested entity was not found\\\",\\\"error\\\":\\\"The model grok-vision-beta does not exist or your team bb642ce4-5161-45c9-8f34-408850883602 does not have access to it. Please ensure you're using the correct API key. If you believe this is a mistake, please contact support and quote your team ID and the model name.\\\"}\",\"provider_name\":\"xAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "anthracite-org/magnum-v2-72b": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"error\\\":{\\\"message\\\":\\\"400: {'error': '/completions: Invalid model name passed in model=anthracite-org-magnum-v2-72b-FP8-Dynamic. Call `/v1/models` to view available models for your key.'}\\\",\\\"type\\\":\\\"None\\\",\\\"param\\\":\\\"None\\\",\\\"code\\\":\\\"400\\\"}}\",\"provider_name\":\"Infermatic\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:56.369Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"error\\\":{\\\"message\\\":\\\"400: {'error': '/completions: Invalid model name passed in model=anthracite-org-magnum-v2-72b-FP8-Dynamic. Call `/v1/models` to view available models for your key.'}\\\",\\\"type\\\":\\\"None\\\",\\\"param\\\":\\\"None\\\",\\\"code\\\":\\\"400\\\"}}\",\"provider_name\":\"Infermatic\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "01-ai/yi-large": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"No endpoints found for 01-ai/yi-large.\",\"code\":404},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:57.668Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"No endpoints found for 01-ai/yi-large.\",\"code\":404},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:1024": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:48:53.999Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:16384": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:48:55.411Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:64000": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:48:56.815Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:8192": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:48:58.812Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:high": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:49:00.221Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:low": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:49:01.628Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:max": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:49:03.288Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-3-7-sonnet:medium": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-07-27T07:49:04.712Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "o3-mini": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:05:57.764Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 3,
      "manuallyBlocked": false
    },
    "o3-mini:high": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:00.373Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 3,
      "manuallyBlocked": false
    },
    "o3-mini:low": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:02.129Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 3,
      "manuallyBlocked": false
    },
    "o3-mini:medium": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:04.065Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 3,
      "manuallyBlocked": false
    },
    "gemini-2.5-pro-preview-03-25": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"{\\n  \\\"error\\\": {\\n    \\\"code\\\": 404,\\n    \\\"message\\\": \\\"Publisher Model `projects/gen-lang-client-0028523586/locations/us-east1/publishers/google/models/gemini-2.5-pro-preview-03-25` not found.\\\",\\n    \\\"status\\\": \\\"NOT_FOUND\\\"\\n  }\\n}\\n\"}}\n",
      "lastTested": "2025-08-07T09:06:49.311Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"{\\n  \\\"error\\\": {\\n    \\\"code\\\": 404,\\n    \\\"message\\\": \\\"Publisher Model `projects/gen-lang-client-0028523586/locations/us-east1/publishers/google/models/gemini-2.5-pro-preview-03-25` not found.\\\",\\n    \\\"status\\\": \\\"NOT_FOUND\\\"\\n  }\\n}\\n\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "gemini-2.5-pro-preview-05-06": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"{\\n  \\\"error\\\": {\\n    \\\"code\\\": 404,\\n    \\\"message\\\": \\\"Publisher Model `projects/gen-lang-client-0028523586/locations/us-east1/publishers/google/models/gemini-2.5-pro-preview-05-06` not found.\\\",\\n    \\\"status\\\": \\\"NOT_FOUND\\\"\\n  }\\n}\\n\"}}\n",
      "lastTested": "2025-08-07T09:06:50.745Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"{\\n  \\\"error\\\": {\\n    \\\"code\\\": 404,\\n    \\\"message\\\": \\\"Publisher Model `projects/gen-lang-client-0028523586/locations/us-east1/publishers/google/models/gemini-2.5-pro-preview-05-06` not found.\\\",\\n    \\\"status\\\": \\\"NOT_FOUND\\\"\\n  }\\n}\\n\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "qwen-qwq-32b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"The model `qwen-qwq-32b` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.\"}}\n",
      "lastTested": "2025-08-07T09:08:33.738Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"The model `qwen-qwq-32b` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "jondurbin/airoboros-l2-70b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:09:38.204Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "google/gemma-2-9b-it": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:10:06.684Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/llama-3.1-70b-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:10:24.694Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/llama-3.1-8b-instruct-bf16": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-07-27T07:52:50.088Z",
      "testError": "HTTP 403: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "meta-llama/llama-3.1-8b-instruct-max": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:10:28.528Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/llama-3.2-11b-vision-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:10:29.961Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "nousresearch/nous-hermes-llama2-13b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:10:57.146Z",
      "testError": "HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "openchat/openchat-7b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:10:58.634Z",
      "testError": "HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "teknium/openhermes-2.5-mistral-7b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:11:00.018Z",
      "testError": "HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "qwen/qwen-2-7b-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:11:01.379Z",
      "testError": "HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "qwen/qwen-2-vl-72b-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:11:02.745Z",
      "testError": "HTTP 500: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "qwen/qwq-32b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "lastTested": "2025-08-07T09:11:10.616Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "gpt-4.5-preview": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"The model `gpt-4.5-preview` does not exist or you do not have access to it.\"}}\n",
      "lastTested": "2025-08-07T09:11:27.623Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"The model `gpt-4.5-preview` does not exist or you do not have access to it.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-07-27T07:53:58.528Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "o1-2024-12-17": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:11:46.200Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o1:high": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:11:51.804Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o1:low": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:11:53.478Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o1:medium": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:11:55.595Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o3": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on requests per min (RPM): Limit 0, Requested 1. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-07-27T07:54:11.257Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on requests per min (RPM): Limit 0, Requested 1. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "o3-2025-04-16": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-08-07T09:11:57.048Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o3-mini-2025-01-31": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:11:58.654Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o3-pro": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-07-27T07:54:23.140Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "o3:flex": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-08-07T09:12:05.603Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o4-mini-2025-04-16": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:07.345Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o4-mini:flex": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:09.393Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o4-mini:high": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:11.017Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o4-mini:low": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:12.651Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "o4-mini:medium": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:12:15.129Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"message\":\"Deployment meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 doesn't exist or isn't accessible.\"}}\n",
      "lastTested": "2025-08-07T09:12:17.270Z",
      "testError": "HTTP 403: {\"error\":{\"message\":\"Deployment meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 doesn't exist or isn't accessible.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/Llama-4-Scout-17B-16E-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"message\":\"Deployment meta-llama/Llama-4-Scout-17B-16E-Instruct doesn't exist or isn't accessible.\"}}\n",
      "lastTested": "2025-08-07T09:12:19.291Z",
      "testError": "HTTP 403: {\"error\":{\"message\":\"Deployment meta-llama/Llama-4-Scout-17B-16E-Instruct doesn't exist or isn't accessible.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "parasail-deepseek-r1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"message\":\"Deployment parasail-deepseek-r1 doesn't exist or isn't accessible.\"}}\n",
      "lastTested": "2025-08-07T09:12:22.217Z",
      "testError": "HTTP 403: {\"error\":{\"message\":\"Deployment parasail-deepseek-r1 doesn't exist or isn't accessible.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "parasail-eva-25-72b-v02-fp8": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"message\":\"Deployment parasail-eva-25-72b-v02-fp8 doesn't exist or isn't accessible.\"}}\n",
      "lastTested": "2025-08-07T09:12:23.579Z",
      "testError": "HTTP 403: {\"error\":{\"message\":\"Deployment parasail-eva-25-72b-v02-fp8 doesn't exist or isn't accessible.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "parasail-mistral-7b-instruct-03": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"message\":\"Deployment parasail-mistral-7b-instruct-03 doesn't exist or isn't accessible.\"}}\n",
      "lastTested": "2025-08-07T09:12:28.031Z",
      "testError": "HTTP 403: {\"error\":{\"message\":\"Deployment parasail-mistral-7b-instruct-03 doesn't exist or isn't accessible.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "parasail-qwen-coder32b-longcontext-128": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"message\":\"Deployment parasail-qwen-coder32b-longcontext-128 doesn't exist or isn't accessible.\"}}\n",
      "lastTested": "2025-07-27T07:54:50.701Z",
      "testError": "HTTP 403: {\"error\":{\"message\":\"Deployment parasail-qwen-coder32b-longcontext-128 doesn't exist or isn't accessible.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepseek-llm-67b-chat": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Unable to access model deepseek-llm-67b-chat. Please visit https://api.together.ai/models to view the list of supported models.\"}}\n",
      "lastTested": "2025-08-07T09:13:03.421Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Unable to access model deepseek-llm-67b-chat. Please visit https://api.together.ai/models to view the list of supported models.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/Llama-2-13b-chat-hf": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Unable to access model meta-llama/Llama-2-13b-chat-hf. Please visit https://api.together.ai/models to view the list of supported models.\"}}\n",
      "lastTested": "2025-08-07T09:13:11.695Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Unable to access model meta-llama/Llama-2-13b-chat-hf. Please visit https://api.together.ai/models to view the list of supported models.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/Llama-2-7b-chat-hf": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Unable to access non-serverless model meta-llama/Llama-2-7b-chat-hf. Please visit https://api.together.ai/models/meta-llama/Llama-2-7b-chat-hf to create and start a new dedicated endpoint for the model.\"}}\n",
      "lastTested": "2025-08-07T09:13:15.809Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Unable to access non-serverless model meta-llama/Llama-2-7b-chat-hf. Please visit https://api.together.ai/models/meta-llama/Llama-2-7b-chat-hf to create and start a new dedicated endpoint for the model.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Unable to access model meta-llama/Meta-Llama-3-70B-Instruct-Lite. Please visit https://api.together.ai/models to view the list of supported models.\"}}\n",
      "lastTested": "2025-08-07T09:13:32.646Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Unable to access model meta-llama/Meta-Llama-3-70B-Instruct-Lite. Please visit https://api.together.ai/models to view the list of supported models.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "upstage/SOLAR-10.7B-Instruct-v1.0": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 503: {\"error\":{\"message\":\"Service unavailable\"}}\n",
      "lastTested": "2025-08-07T09:13:59.678Z",
      "testError": "HTTP 503: {\"error\":{\"message\":\"Service unavailable\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "sarvamai/sarvam-m:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: sarvamai/sarvam-m\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:33:59.416Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider returned error\",\"code\":404,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"model not found: sarvamai/sarvam-m\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "thudm/glm-z1-32b": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"No endpoints found for thudm/glm-z1-32b.\",\"code\":404},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:34:06.389Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"No endpoints found for thudm/glm-z1-32b.\",\"code\":404},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "sao10k/fimbulvetr-11b-v2": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"statusCode\\\":400,\\\"message\\\":\\\"The model is not ready for inference. Please wait for the model to warm, then try again.\\\",\\\"code\\\":\\\"model_pending_deploy\\\"}\",\"provider_name\":\"Featherless\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:34:13.809Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"statusCode\\\":400,\\\"message\\\":\\\"The model is not ready for inference. Please wait for the model to warm, then try again.\\\",\\\"code\\\":\\\"model_pending_deploy\\\"}\",\"provider_name\":\"Featherless\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "undi95/toppy-m-7b": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"statusCode\\\":400,\\\"message\\\":\\\"The model is not ready for inference. Please wait for the model to warm, then try again.\\\",\\\"code\\\":\\\"model_pending_deploy\\\"}\",\"provider_name\":\"Featherless\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-08-07T09:34:15.554Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"statusCode\\\":400,\\\"message\\\":\\\"The model is not ready for inference. Please wait for the model to warm, then try again.\\\",\\\"code\\\":\\\"model_pending_deploy\\\"}\",\"provider_name\":\"Featherless\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "claude-4-opus@us-west-2": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "lastTested": "2025-08-07T09:04:14.201Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-4-sonnet@eu-central-1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "lastTested": "2025-08-07T09:05:18.971Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-4-sonnet@eu-north-1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "lastTested": "2025-08-07T09:05:24.088Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-4-sonnet@eu-west-1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "lastTested": "2025-08-07T09:05:27.980Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "claude-4-sonnet@eu-west-3": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "lastTested": "2025-08-07T09:05:32.650Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"{\\\"message\\\":\\\"Too many requests, please wait before trying again.\\\"}\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepseek-ai/DeepSeek-V3-0324-fast": {
      "provider": "requesty",
      "reason": "Endpoint test failed: Request timeout",
      "lastTested": "2025-08-07T09:09:26.790Z",
      "testError": "Request timeout",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/google/gemini-2.5-flash-preview-05-20": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:05.337Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "anthropic/claude-3-7-sonnet": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:15.521Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 8,
      "manuallyBlocked": false
    },
    "google/gemini-2.5-pro-preview-03-25": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:27.810Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "google/gemini-2.5-pro-preview-05-06": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:29.020Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/qwen-qwq-32b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:30.226Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/jondurbin/airoboros-l2-70b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:31.423Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/google/gemma-2-9b-it": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:32.648Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/llama-3.1-70b-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:33.859Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/llama-3.1-8b-instruct-bf16": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:35.445Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/llama-3.1-8b-instruct-max": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:36.647Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/llama-3.2-11b-vision-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:37.868Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/nousresearch/nous-hermes-llama2-13b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:39.065Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/openchat/openchat-7b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:40.258Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/teknium/openhermes-2.5-mistral-7b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:41.444Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/qwen/qwen-2-7b-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:42.645Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/qwen/qwen-2-vl-72b-instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:43.840Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/qwen/qwq-32b": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:39:45.028Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/gpt-4.5-preview": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"The model `gpt-4.5-preview` does not exist or you do not have access to it.\"}}\n",
      "lastTested": "2025-08-07T09:39:46.507Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"The model `gpt-4.5-preview` does not exist or you do not have access to it.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/o3-2025-04-16": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-08-07T09:40:04.132Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on tokens per min (TPM): Limit 0, Requested 4. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/o3:flex": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on requests per min (RPM): Limit 0, Requested 1. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "lastTested": "2025-08-07T09:40:09.747Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Request too large for o3 in organization org-lLOq0ZcXUOSmEIdMWqYdwRFY on requests per min (RPM): Limit 0, Requested 1. Visit https://platform.openai.com/account/rate-limits to learn more.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/o4-mini-2025-04-16": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-08-07T09:40:12.113Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:26.347Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/Llama-4-Scout-17B-16E-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:27.546Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/parasail-deepseek-r1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:28.748Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/parasail-eva-25-72b-v02-fp8": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:29.958Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/parasail-mistral-7b-instruct-03": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:31.171Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/parasail-qwen-coder32b-longcontext-128": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:32.354Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/deepseek-llm-67b-chat": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:33.550Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/Llama-2-13b-chat-hf": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:34.763Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/Llama-2-7b-chat-hf": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:35.981Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:37.185Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/upstage/SOLAR-10.7B-Instruct-v1.0": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:38.369Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "anthropic/claude-4-opus@us-west-2": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:39.577Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "anthropic/claude-4-sonnet@eu-central-1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:40.763Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "anthropic/claude-4-sonnet@eu-north-1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:41.953Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "anthropic/claude-4-sonnet@eu-west-1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:43.153Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "anthropic/claude-4-sonnet@eu-west-3": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:45.401Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/deepseek-ai/DeepSeek-V3-0324-fast": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "lastTested": "2025-08-07T09:40:46.587Z",
      "testError": "HTTP 404: {\"error\":{\"message\":\"Provider and/or model not supported\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/gpt-5-image-mini": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unsupported parameter: 'temperature' is not supported with this model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"temperature\\\",\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"OpenAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:18:56.341Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unsupported parameter: 'temperature' is not supported with this model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"temperature\\\",\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"OpenAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/gpt-5-image": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unsupported parameter: 'temperature' is not supported with this model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"temperature\\\",\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"OpenAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:19:08.482Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unsupported parameter: 'temperature' is not supported with this model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"temperature\\\",\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"OpenAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/o3-deep-research": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"This model is only supported in v1/responses and not in v1/chat/completions.\"}}\n",
      "lastTested": "2025-12-07T09:07:27.487Z",
      "testError": "HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"This model is only supported in v1/responses and not in v1/chat/completions.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "openai/o4-mini-deep-research": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"This model is only supported in v1/responses and not in v1/chat/completions.\"}}\n",
      "lastTested": "2025-12-07T09:07:40.909Z",
      "testError": "HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"This model is only supported in v1/responses and not in v1/chat/completions.\"}}\n",
      "retryCount": 2,
      "manuallyBlocked": false
    },
    "relace/relace-apply-3": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"expected `<code>...</code>` and `<update>...</update>` tags\",\"provider_name\":\"Relace\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:20:14.806Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"expected `<code>...</code>` and `<update>...</update>` tags\",\"provider_name\":\"Relace\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "alibaba/tongyi-deepresearch-30b-a3b:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 502: {\"error\":{\"message\":\"Provider returned error\",\"code\":502,\"metadata\":{\"raw\":\"<html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:20:51.463Z",
      "testError": "HTTP 502: {\"error\":{\"message\":\"Provider returned error\",\"code\":502,\"metadata\":{\"raw\":\"<html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n<hr><center>nginx</center>\\r\\n</body>\\r\\n</html>\\r\\n\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "meituan/longcat-flash-chat:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"No instances available (yet) for chute_id='2a6173bd-6d60-5ca6-8601-9ece77f055e4'\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:21:05.354Z",
      "testError": "HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"No instances available (yet) for chute_id='2a6173bd-6d60-5ca6-8601-9ece77f055e4'\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepcogito/cogito-v2-preview-deepseek-671b": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNETnMA-2kFHot-9aa29c13b3c0a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unable to access non-serverless model deepcogito/cogito-v2-preview-deepseek-671b. Please visit https://api.together.ai/models/deepcogito/cogito-v2-preview-deepseek-671b to create and start a new dedicated endpoint for the model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": \\\"model_not_available\\\"\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:21:29.542Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNETnMA-2kFHot-9aa29c13b3c0a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unable to access non-serverless model deepcogito/cogito-v2-preview-deepseek-671b. Please visit https://api.together.ai/models/deepcogito/cogito-v2-preview-deepseek-671b to create and start a new dedicated endpoint for the model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": \\\"model_not_available\\\"\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/gpt-4o-audio-preview": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"This model requires that either input content or output modality contain audio.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"model\\\",\\n    \\\"code\\\": \\\"invalid_value\\\"\\n  }\\n}\",\"provider_name\":\"OpenAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:40:42.753Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"This model requires that either input content or output modality contain audio.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": \\\"model\\\",\\n    \\\"code\\\": \\\"invalid_value\\\"\\n  }\\n}\",\"provider_name\":\"OpenAI\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "qwen/qwen3-coder:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}",
      "lastTested": "2025-12-07T08:42:18.288Z",
      "testError": "HTTP 500: {\"error\":{\"message\":\"Internal Server Error\",\"code\":500}}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "cognitivecomputations/dolphin-mistral-24b-venice-edition:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"cognitivecomputations/dolphin-mistral-24b-venice-edition:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:42:46.150Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"cognitivecomputations/dolphin-mistral-24b-venice-edition:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "arcee-ai/spotlight": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbQHA-2kFHot-9aa2c05f31b3a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Service unavailable\\\",\\n    \\\"type\\\": \\\"service_unavailable\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:46:16.264Z",
      "testError": "HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbQHA-2kFHot-9aa2c05f31b3a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Service unavailable\\\",\\n    \\\"type\\\": \\\"service_unavailable\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "arcee-ai/maestro-reasoning": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbQtt-2kFHot-9aa2c06e8504a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Service unavailable\\\",\\n    \\\"type\\\": \\\"service_unavailable\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:46:18.311Z",
      "testError": "HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbQtt-2kFHot-9aa2c06e8504a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Service unavailable\\\",\\n    \\\"type\\\": \\\"service_unavailable\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "arcee-ai/virtuoso-large": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbRWb-2kFHot-9aa2c07b5028a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Service unavailable\\\",\\n    \\\"type\\\": \\\"service_unavailable\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:46:20.406Z",
      "testError": "HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbRWb-2kFHot-9aa2c07b5028a943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Service unavailable\\\",\\n    \\\"type\\\": \\\"service_unavailable\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "arcee-ai/coder-large": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbS8X-2kFHot-9aa2c088732da943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unable to access non-serverless model arcee-ai/coder-large. Please visit https://api.together.ai/models/arcee-ai/coder-large to create and start a new dedicated endpoint for the model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": \\\"model_not_available\\\"\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:46:22.460Z",
      "testError": "HTTP 400: {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\n  \\\"id\\\": \\\"oNEbS8X-2kFHot-9aa2c088732da943\\\",\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Unable to access non-serverless model arcee-ai/coder-large. Please visit https://api.together.ai/models/arcee-ai/coder-large to create and start a new dedicated endpoint for the model.\\\",\\n    \\\"type\\\": \\\"invalid_request_error\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": \\\"model_not_available\\\"\\n  }\\n}\",\"provider_name\":\"Together\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "qwen/qwen3-4b:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"qwen/qwen3-4b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:46:27.894Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"qwen/qwen3-4b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "qwen/qwen3-235b-a22b:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"qwen/qwen3-235b-a22b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:46:47.891Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"qwen/qwen3-235b-a22b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "microsoft/mai-ds-r1": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"No instances available (yet) for chute_id='44e8f7a0-67f3-51a2-bd4a-5f75b88a9132'\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:47:05.244Z",
      "testError": "HTTP 503: {\"error\":{\"message\":\"Provider returned error\",\"code\":503,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"No instances available (yet) for chute_id='44e8f7a0-67f3-51a2-bd4a-5f75b88a9132'\\\"}\",\"provider_name\":\"Chutes\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "eleutherai/llemma_7b": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"eleutherai/llemma_7b is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Featherless\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:47:27.081Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"eleutherai/llemma_7b is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Featherless\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepseek/deepseek-r1-distill-qwen-14b": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: Invalid response format - missing choices array",
      "lastTested": "2025-12-07T08:52:20.047Z",
      "testError": "Invalid response format - missing choices array",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "google/gemini-2.0-flash-exp:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"google/gemini-2.0-flash-exp:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Google\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:53:05.187Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"google/gemini-2.0-flash-exp:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Google\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "anthropic/claude-3.5-haiku-20241022": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"anthropic/claude-3.5-haiku-20241022 is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Google\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:53:36.521Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"anthropic/claude-3.5-haiku-20241022 is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Google\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "nousresearch/hermes-3-llama-3.1-405b:free": {
      "provider": "openrouter",
      "reason": "Endpoint test failed: HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"nousresearch/hermes-3-llama-3.1-405b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "lastTested": "2025-12-07T08:54:39.603Z",
      "testError": "HTTP 429: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"nousresearch/hermes-3-llama-3.1-405b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_2ly1KOnY3h2wVmQvG7eELOtIaz1\"}",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "azure/gpt-5.1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.\"}}\n",
      "lastTested": "2025-12-07T08:58:15.622Z",
      "testError": "HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "azure/o4-mini@uksouth": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.\"}}\n",
      "lastTested": "2025-12-07T08:58:34.527Z",
      "testError": "HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "azure/o4-mini@westus3": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "lastTested": "2025-12-07T08:58:36.534Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:1024": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:15.024Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:16384": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:16.479Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:64000": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:17.918Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:8192": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:19.628Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:high": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:21.046Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:low": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:22.557Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:max": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:24.164Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "coding/claude-3-7-sonnet-20250219:medium": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "lastTested": "2025-12-07T09:01:25.594Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"`max_tokens` must be greater than `thinking.budget_tokens`. Please consult our documentation at https://docs.claude.com/en/docs/build-with-claude/extended-thinking#max-tokens-and-context-window-size\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/deepseek-ai/DeepSeek-V3.1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:02:48.598Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/meta-llama/Llama-3.3-70B-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:02:52.443Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/meta-llama/Llama-3.3-70B-Instruct-Turbo": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:02:53.888Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/meta-llama/Meta-Llama-3.1-405B-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:02:55.319Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"[TensorRT-LLM][ERROR] Assertion failed: mMaxNewTokens \\u003e 0 (/home/jenkins/agent/workspace/LLM/helpers/Build-x86_64/llm/cpp/tensorrt_llm/executor/requestImpl.h:382)\\n1       0x7fe43d0975c6 tensorrt_llm::common::throwRuntimeError(char const*, int, std::string const\\u0026) + 102\\n2       0x7fe43f518369 /app/tensorrt_llm/cpp/build/tensorrt_llm/libtensorrt_llm.so(+0x2bfd369) [0x7fe43f518369]\\n3       0x7fe43f5187fa tensorrt_llm::executor::Request::Request(std::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e, int, bool, tensorrt_llm::executor::SamplingConfig const\\u0026, tensorrt_llm::executor::OutputConfig const\\u0026, std::optional\\u003cint\\u003e const\\u0026, std::optional\\u003cint\\u003e const\\u0026, std::optional\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e, std::optional\\u003cstd::list\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e, std::allocator\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e \\u003e \\u003e, std::optional\\u003cstd::list\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e, std::allocator\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e \\u003e \\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e, std::optional\\u003ctensorrt_llm::executor::ExternalDraftTokensConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::PromptTuningConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::LoraConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::LookaheadDecodingConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::KvCacheRetentionConfig\\u003e, std::optional\\u003cstd::string\\u003e, std::optional\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e, std::optional\\u003cunsigned long\\u003e, bool, float, tensorrt_llm::executor::RequestType, std::optional\\u003ctensorrt_llm::executor::ContextPhaseParams\\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e, std::optional\\u003cint\\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e, int, std::optional\\u003ctensorrt_llm::executor::EagleConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e) + 202\\n4       0x7fe530550a18 triton::backend::inflight_batcher_llm::utils::createRequestsFromInputTensors(std::vector\\u003cstd::unordered_map\\u003cstd::string, tensorrt_llm::batch_manager::NamedTensor, std::hash\\u003cstd::string\\u003e, std::equal_to\\u003cstd::string\\u003e, std::allocator\\u003cstd::pair\\u003cstd::string const, tensorrt_llm::batch_manager::NamedTensor\\u003e \\u003e \\u003e, std::allocator\\u003cstd::unordered_map\\u003cstd::string, tensorrt_llm::batch_manager::NamedTensor, std::hash\\u003cstd::string\\u003e, std::equal_to\\u003cstd::string\\u003e, std::allocator\\u003cstd::pair\\u003cstd::string const, tensorrt_llm::batch_manager::NamedTensor\\u003e \\u003e \\u003e \\u003e \\u003e const\\u0026, bool, bool, bool, tensorrt_llm::executor::ModelType, tensorrt_llm::executor::RequestType, bool, bool, triton::backend::inflight_batcher_llm::StructuredBatchedLogitProcessor*, std::string const\\u0026, std::vector\\u003cstd::unique_ptr\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder, std::default_delete\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder\\u003e \\u003e, std::allocator\\u003cstd::unique_ptr\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder, std::default_delete\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder\\u003e \\u003e \\u003e \\u003e\\u0026) + 3384\\n5       0x7fe530503196 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x28196) [0x7fe530503196]\\n6       0x7fe530515639 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x3a639) [0x7fe530515639]\\n7       0x7fe5304f8f65 TRITONBACKEND_ModelInstanceExecute + 101\\n8       0x7fe7441070b4 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a70b4) [0x7fe7441070b4]\\n9       0x7fe74410742b /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a742b) [0x7fe74410742b]\\n10      0x7fe744225ccd /opt/tritonserver/bin/../lib/libtritonserver.so(+0x2c5ccd) [0x7fe744225ccd]\\n11      0x7fe74410b864 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1ab864) [0x7fe74410b864]\\n12      0x7fe7439cc253 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7fe7439cc253]\\n13      0x7fe74375bac3 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7fe74375bac3]\\n14      0x7fe7437ed850 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7fe7437ed850]\"}}\n",
      "lastTested": "2025-12-07T09:02:57.360Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"[TensorRT-LLM][ERROR] Assertion failed: mMaxNewTokens \\u003e 0 (/home/jenkins/agent/workspace/LLM/helpers/Build-x86_64/llm/cpp/tensorrt_llm/executor/requestImpl.h:382)\\n1       0x7fe43d0975c6 tensorrt_llm::common::throwRuntimeError(char const*, int, std::string const\\u0026) + 102\\n2       0x7fe43f518369 /app/tensorrt_llm/cpp/build/tensorrt_llm/libtensorrt_llm.so(+0x2bfd369) [0x7fe43f518369]\\n3       0x7fe43f5187fa tensorrt_llm::executor::Request::Request(std::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e, int, bool, tensorrt_llm::executor::SamplingConfig const\\u0026, tensorrt_llm::executor::OutputConfig const\\u0026, std::optional\\u003cint\\u003e const\\u0026, std::optional\\u003cint\\u003e const\\u0026, std::optional\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e, std::optional\\u003cstd::list\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e, std::allocator\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e \\u003e \\u003e, std::optional\\u003cstd::list\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e, std::allocator\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e \\u003e \\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e, std::optional\\u003ctensorrt_llm::executor::ExternalDraftTokensConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::PromptTuningConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::LoraConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::LookaheadDecodingConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::KvCacheRetentionConfig\\u003e, std::optional\\u003cstd::string\\u003e, std::optional\\u003cstd::vector\\u003cint, std::allocator\\u003cint\\u003e \\u003e \\u003e, std::optional\\u003cunsigned long\\u003e, bool, float, tensorrt_llm::executor::RequestType, std::optional\\u003ctensorrt_llm::executor::ContextPhaseParams\\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e, std::optional\\u003cint\\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e, int, std::optional\\u003ctensorrt_llm::executor::EagleConfig\\u003e, std::optional\\u003ctensorrt_llm::executor::Tensor\\u003e) + 202\\n4       0x7fe530550a18 triton::backend::inflight_batcher_llm::utils::createRequestsFromInputTensors(std::vector\\u003cstd::unordered_map\\u003cstd::string, tensorrt_llm::batch_manager::NamedTensor, std::hash\\u003cstd::string\\u003e, std::equal_to\\u003cstd::string\\u003e, std::allocator\\u003cstd::pair\\u003cstd::string const, tensorrt_llm::batch_manager::NamedTensor\\u003e \\u003e \\u003e, std::allocator\\u003cstd::unordered_map\\u003cstd::string, tensorrt_llm::batch_manager::NamedTensor, std::hash\\u003cstd::string\\u003e, std::equal_to\\u003cstd::string\\u003e, std::allocator\\u003cstd::pair\\u003cstd::string const, tensorrt_llm::batch_manager::NamedTensor\\u003e \\u003e \\u003e \\u003e \\u003e const\\u0026, bool, bool, bool, tensorrt_llm::executor::ModelType, tensorrt_llm::executor::RequestType, bool, bool, triton::backend::inflight_batcher_llm::StructuredBatchedLogitProcessor*, std::string const\\u0026, std::vector\\u003cstd::unique_ptr\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder, std::default_delete\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder\\u003e \\u003e, std::allocator\\u003cstd::unique_ptr\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder, std::default_delete\\u003ctriton::backend::inflight_batcher_llm::FreeStateHolder\\u003e \\u003e \\u003e \\u003e\\u0026) + 3384\\n5       0x7fe530503196 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x28196) [0x7fe530503196]\\n6       0x7fe530515639 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x3a639) [0x7fe530515639]\\n7       0x7fe5304f8f65 TRITONBACKEND_ModelInstanceExecute + 101\\n8       0x7fe7441070b4 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a70b4) [0x7fe7441070b4]\\n9       0x7fe74410742b /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a742b) [0x7fe74410742b]\\n10      0x7fe744225ccd /opt/tritonserver/bin/../lib/libtritonserver.so(+0x2c5ccd) [0x7fe744225ccd]\\n11      0x7fe74410b864 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1ab864) [0x7fe74410b864]\\n12      0x7fe7439cc253 /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7fe7439cc253]\\n13      0x7fe74375bac3 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7fe74375bac3]\\n14      0x7fe7437ed850 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7fe7437ed850]\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:02:58.800Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/microsoft/phi-4": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:03:00.904Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/Qwen/Qwen2.5-72B-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:03:02.312Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/Qwen/Qwen2.5-Coder-32B-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:03:03.736Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/Qwen/Qwen3-32B": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "lastTested": "2025-12-07T09:03:08.374Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference error\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "deepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference exception\"}}\n",
      "lastTested": "2025-12-07T09:03:09.793Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"inference exception\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai/o1-pro": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"This model is only supported in v1/responses and not in v1/chat/completions.\"}}\n",
      "lastTested": "2025-12-07T09:07:16.584Z",
      "testError": "HTTP 404: {\"error\":{\"origin\":\"provider\",\"message\":\"This model is only supported in v1/responses and not in v1/chat/completions.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "openai-responses/gpt-5-pro": {
      "provider": "requesty",
      "reason": "Endpoint test failed: Request timeout",
      "lastTested": "2025-12-07T09:08:42.294Z",
      "testError": "Request timeout",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "parasail/parasail-glm-45": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 403: {\"error\":{\"origin\":\"provider\",\"message\":\"Deployment zai-org/GLM-4.5V doesn't exist or isn't accessible.\"}}\n",
      "lastTested": "2025-12-07T09:09:12.341Z",
      "testError": "HTTP 403: {\"error\":{\"origin\":\"provider\",\"message\":\"Deployment zai-org/GLM-4.5V doesn't exist or isn't accessible.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "together/Qwen/Qwen2.5-Coder-32B-Instruct": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"Unable to access non-serverless model Qwen/Qwen2.5-Coder-32B-Instruct. Please visit https://api.together.ai/models/Qwen/Qwen2.5-Coder-32B-Instruct to create and start a new dedicated endpoint for the model.\"}}\n",
      "lastTested": "2025-12-07T09:10:08.955Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"Unable to access non-serverless model Qwen/Qwen2.5-Coder-32B-Instruct. Please visit https://api.together.ai/models/Qwen/Qwen2.5-Coder-32B-Instruct to create and start a new dedicated endpoint for the model.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "together/Qwen/QwQ-32B-Preview": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"Unable to access non-serverless model Qwen/QwQ-32B-Preview. Please visit https://api.together.ai/models/Qwen/QwQ-32B-Preview to create and start a new dedicated endpoint for the model.\"}}\n",
      "lastTested": "2025-12-07T09:10:11.441Z",
      "testError": "HTTP 400: {\"error\":{\"origin\":\"provider\",\"message\":\"Unable to access non-serverless model Qwen/QwQ-32B-Preview. Please visit https://api.together.ai/models/Qwen/QwQ-32B-Preview to create and start a new dedicated endpoint for the model.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "vertex/claude-opus-4-1@europe-west1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"Internal error encountered.\"}}\n",
      "lastTested": "2025-12-07T09:10:46.475Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"Internal error encountered.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    },
    "vertex/claude-opus-4@europe-west1": {
      "provider": "requesty",
      "reason": "Endpoint test failed: HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"Internal error encountered.\"}}\n",
      "lastTested": "2025-12-07T09:11:00.769Z",
      "testError": "HTTP 500: {\"error\":{\"origin\":\"provider\",\"message\":\"Internal error encountered.\"}}\n",
      "retryCount": 1,
      "manuallyBlocked": false
    }
  }
}